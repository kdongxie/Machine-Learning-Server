# 데이터 출력&모델성능평가&결정트리&그래프 

## 필요 Package

```python
#!pip install pydataset
#https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html
```

```python
import os
import graphviz
os.environ["PATH"] += os.pathsep + 'C:\\Program Files (x86)\\Graphviz2.38\\bin'
from sklearn import datasets
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier
from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.utils.multiclass import unique_labels
from sklearn.metrics import accuracy_score
from sklearn import metrics
import pandas as pd
import numpy as np
import itertools 
import matplotlib.pyplot as plt 
from sklearn import svm, datasets 
# ! pip install scikit-plot
import scikitplot as skplt
```

### pydataset 을 통한 data호출

```python
#!pip install pydataset
from pydataset import data
# Get the housing data
df = data('Housing')
df.head().values
```

```python
array([[42000.0, 5850, 3, 1, 2, 'yes', 'no', 'yes', 'no', 'no', 1, 'no'],
       [38500.0, 4000, 2, 1, 1, 'yes', 'no', 'no', 'no', 'no', 0, 'no'],
       [49500.0, 3060, 3, 1, 1, 'yes', 'no', 'no', 'no', 'no', 0, 'no'],
       [60500.0, 6650, 3, 1, 2, 'yes', 'yes', 'no', 'no', 'no', 0, 'no'],
       [61000.0, 6360, 2, 1, 1, 'yes', 'no', 'no', 'no', 'no', 0, 'no']],
      dtype=object)
```

### ZIP 함수를 사용하여 데이터를 묶어준다

```python
# label 'yes' and 'no'
d = dict(zip(['no', 'yes'], range(0,2)))
for i in zip(df.dtypes.index, df.dtypes):
    if str(i[1]) == 'object':
        df[i[0]] = df[i[0]].map(d)
        
# zip(*iterable)은 동일한 개수로 이루어진 자료형을 묶어 주는 역할을 하는 함수이다.
```

### enumerate 함수 사용하여 연속데이터를 범위 데이터로 변경 시켜 준다

>enumerate는 "열거하다"라는 뜻이다. 이 함수는 순서가 있는 자료형(리스트, 튜플, 문자열)을 입력으로 받아 인덱스 값을 포함하는 enumerate 객체를 리턴한다.
>
>※ 보통 enumerate 함수는 아래 예제처럼 for문과 함께 자주 사용된다.

```python
for i, j in enumerate(np.unique(pd.qcut(df['price'], 3))):
    print(i, j)
```

```python
0 (24999.999, 53000.0]
1 (53000.0, 74500.0]
2 (74500.0, 190000.0]
```

### 데이터를 나누어 두개의 변수로 저장해 준다.

```python
df['price'] = pd.qcut(df['price'], 3, labels=['0', '1', '2']).cat.codes
y = df['price']
y1= df['prefarea']
x = df.drop('price', 1)
x1 = df.drop(['prefarea'],1)
```

### 데이터를 확인

```python
print(type(y)), print(type(x))
x.head()
```

![image](https://user-images.githubusercontent.com/46669551/55712818-61f3ba80-5a2a-11e9-97ed-be326d182014.png)

```python
print(type(y)), print(type(x))
y.head()
```

![image](https://user-images.githubusercontent.com/46669551/55712884-7e8ff280-5a2a-11e9-8045-7246970249fe.png)

> 이 데이터를 나누는 이유는 x변수에 담겨져 있는 각각의 변수들이 y변수에 담겨진 price에 미치는 영향에 대하여 알아보고자 함에 있다. 따라서 y = a1x1+a2x2+...anxn 의 다변량 함수가 이루어 진다.

### 각각의 데이터의 형태를 확인

```python
x.shape, y.shape
```

```python
((546, 11), (546,))
```

### 머신러닝을 위하여 학습데이터와 검증데이터로 나눈다.

```python
xtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size=0.3, random_state=0)
xtest
```

![image](https://user-images.githubusercontent.com/46669551/55713138-00801b80-5a2b-11e9-9e4d-41418d25b48a.png)

```python
ytest
```

![image](https://user-images.githubusercontent.com/46669551/55713187-1988cc80-5a2b-11e9-971c-b58ce23e2942.png)

### 데이터 검증 함수를 선언 시켜준다

```python
logReg = LogisticRegression(solver="liblinear") # 로지스틱 검증
svm = SVC(gamma='auto') # 서포트 벡터 머신
knn = KNeighborsClassifier() # Nearest Neighbor 검증
rf = RandomForestClassifier(n_estimators=10) # 렌덤포레스트 검증
dt = DecisionTreeClassifier() # 결정트리 검증
classify = [("LR",logReg),("KNN",knn),("DT",dt), ("RFC",rf),("SVM",svm)] #하나의 변수에 담기 
classify
```

```python
[('LR',
  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
            intercept_scaling=1, max_iter=100, multi_class='warn',
            n_jobs=None, penalty='l2', random_state=None, solver='liblinear',
            tol=0.0001, verbose=0, warm_start=False)),
 ('KNN',
  KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
             metric_params=None, n_jobs=None, n_neighbors=5, p=2,
             weights='uniform')),
 ('DT',
  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, presort=False, random_state=None,
              splitter='best')),
 ('RFC',
  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
              max_depth=None, max_features='auto', max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,
              oob_score=False, random_state=None, verbose=0,
              warm_start=False)),
 ('SVM', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False))]
```

### VotingClassifier 를 사용하여 각각의 머신들을 비교하여 최적의 결과측정 머신을 선택하기

```python
vc = VotingClassifier(estimators=classify,voting='hard')
vc.fit(xtrain, ytrain)
```

```python
C:\Users\Administrator\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.
  "this warning.", FutureWarning)
VotingClassifier(estimators=[('LR', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',
          tol=0.0001, verbose=0, warm_start=False)), ('KNN', KN...,
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False))],
         flatten_transform=None, n_jobs=None, voting='hard', weights=None)
```

### for문을 이용하여 각각의 머신에 검증데이터를 돌린다.

```python
for clf in (logReg, svm,knn,rf,dt,vc):
    clf.fit(xtrain, ytrain)
    clfResult = clf.predict(xtest)
    print(clf.__class__.__name__, accuracy_score(ytest,clfResult))
```

```python
C:\Users\Administrator\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.
  "this warning.", FutureWarning)
############################################
LogisticRegression 0.5975609756097561      #
SVC 0.4329268292682927                     #
KNeighborsClassifier 0.524390243902439     #
RandomForestClassifier 0.7012195121951219  # <== 이부분을 통해 확인 가능하다.
DecisionTreeClassifier 0.5853658536585366  #
VotingClassifier 0.6097560975609756        #
############################################
C:\Users\Administrator\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.
  "this warning.", FutureWarning)
```

### Decission Tree 를 그려보자

```python
dtree = tree.DecisionTreeClassifier(criterion="entropy", 
                                    max_depth=3, 
                                    random_state=0)
xtrain1,xtest1,ytrain1,ytest1 = train_test_split(x1,y1,test_size=0.3,random_state=0)
dtree.fit(xtrain1,ytrain1)
treeResult = dtree.predict(xtest1)
dot_data = tree.export_graphviz(dtree,
                               out_file=None, filled=True, rounded=True,
                               feature_names=list(df.columns.values[:-1]))
graph = graphviz.Source(dot_data)
graph
```

![image](https://user-images.githubusercontent.com/46669551/55713748-44275500-5a2c-11e9-9861-9df7b1ce5ece.png)

### Cconfusion Metrix를 그리기

```python
skplt.metrics.plot_confusion_matrix( ytest,clfResult)
# 정규화 : normalize=True
```

![image](https://user-images.githubusercontent.com/46669551/55713820-6b7e2200-5a2c-11e9-804e-87a37391525c.png)

### ETC. 예쁘게 출력하기

```python
c="훈련 세트 정확도: {:.3f}".format(rf.score(xtrain, ytrain))
d="테스트 세트 정확도: {:.3f}".format(rf.score(xtest, ytest))
```

```python
a = "Logistic Regression 훈련 세트 정확도: {:.3f}".format(logReg.score(xtrain, ytrain))
b = "Logistic Regression 테스트 세트 정확도: {:.3f}".format(logReg.score(xtest, ytest))
```

```python
datafram = pd.DataFrame({"sas":[a,b],'sss':[c,d]})
```

```python
datafram
```

![image](https://user-images.githubusercontent.com/46669551/55713921-a08a7480-5a2c-11e9-8d8b-cb3bedf76cf4.png)

